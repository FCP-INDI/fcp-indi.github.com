Benchmark Package
=================

The C-PAC benchmark package consists of all of the data files, configuration files, and scripts needed to test C-PAC on your machine and compare your outputs with a standard set of outputs processed by the C-PAC team. Specifically, it contains:

* A bash script to automatically configure the paths within all of the settings files (``adjustSettingPaths.sh``).

* Anatomical and functional images for one subject from the ADHD-200 dataset.

* A pipeline configuration file using ANTS (``pipeline_config_ANTS_example.yml``).

* A subject list (``CPAC_subject_list_1sub.yml``).

* Masks, ROIs, spatial maps, and seed specifications (for timeseries extraction, centrality, etc).

* A Python script to compare your outputs with ours (``correlations_workflow_pipeline.py``).

The following instructions will guide you through the process of running the C-PAC benchmark. Before you begin, make sure that you have at least 24 GB of RAM.  Also, note that if you chose not to install ANTS along with C-PAC, the benchmark will not work properly.  Click the links below to download the package and the pre-computed outputs for C-PAC 0.3.9.2:

* `The C-PAC Benchmark Package <https://s3.amazonaws.com/fcp-indi/data/test_resources/benchmark_package.tar.gz>`_

* `Precomputed Results for an Individual-Level Analysis Using C-PAC 0.3.9.2 <https://s3.amazonaws.com/fcp-indi/data/test_resources/output_v0392.tar.gz>`_

Once downloaded, these packages should be extracted using the command ``tar xzvf <filename>``.  You may also want to consider moving `output_v0392` into the main `benchmark_package` directory to keep things organized.

Configuring C-PAC
^^^^^^^^^^^^^^^^^
First, navigate to ``/scripts`` and execute ``./adjustSettingPaths.sh``.  This will ensure that the paths used in the configuration files are in accord with whichever directory you choose to store ``benchmark_package`` in.  This script assumes that it lives in the ``/scripts`` directory.  If it has been moved elsewhere, it may be executed using ``./adjustSettingPaths.sh <path to benchmark_package directory>``

Next, load in the single subject from the ADHD-200 dataset.  Open the C-PAC GUI and click `Load` next to the subject pane. Select ``CPAC_subject_list_1sub.yml``.

Finally, load in the pipeline configuration.  In the main C-PAC window, under `Pipelines`, click `Load` and select the ``pipeline_config_ANTS_example.yml`` file located in ``/settings/configs`` in the benchmark directory. A new pipeline will show up in the list window.  Select this pipeline and click `Edit` to inspect it.

Running An Individual-Level Analyses
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Once you have configured C-PAC by loading in the pipeline configuration YAML and subject list, you may run the individual-level analysis pipeline by clicking the appropriate button at the bottom of the main C-PAC window. This will bring up a log window that displays the progress of the C-PAC run. On an Intel Core i7 2.4 GHz laptop with 8 cores and 16 GB RAM, the pipeline finishes executing in around 40 minutes.

Comparing Outputs
^^^^^^^^^^^^^^^^^^
Navigate to the ``/scripts`` directory of the C-PAC Benchmark Package. Here you will find a script named ``correlations_workflow_pipeline.py``, which can help you compare your outputs and the reference outputs from the C-PAC team.

This will produce Pearson's r and a concordance correlation coefficient between two sets of output for the following measures:

* ALFF and f/ALFF
* Centrality
* Dual Regression
* ReHo
* VMHC

To run this script, type ``python correlations_workflow_pipeline.py <path to output1 pipeline> <path to output2 pipeline> <number of cores to use> <descriptive name>``.  Make sure that the paths you use point to the ``pipeline`` directories within the output directories.  To determine the maximum number of cores on your machine, type ``nproc`` in a terminal window.

When this script is done, it will produce plots for the correlation measures, stored in the directories 'MNI_outputs', 'MNI_SCA', 'native_outputs', 'natived_SCA', 'nonscrub_vs_scrub', and 'registration'.  It also produces Python pickle files containing dictionaries with measures as keys and Pearson or concordance correlations as values. Python pickles can be thought of as similar to .mat files in MATLAB- they allow you to save variables in the workspace to an external file on the hard drive. 
